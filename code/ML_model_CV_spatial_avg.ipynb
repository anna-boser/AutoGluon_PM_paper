{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfSWDfztoaJ0"
      },
      "source": [
        "# Set up Autogluon and get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "oNx9F7i2oKIB",
        "outputId": "9229c564-50ef-4d8b-e635-85ef210c9f26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/updated_autogluon/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from autogluon.tabular import TabularPredictor, TabularDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import gc\n",
        "import os\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ysZqDetyoOoD"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unique</th>\n",
              "      <th>Id</th>\n",
              "      <th>Day</th>\n",
              "      <th>Lat</th>\n",
              "      <th>Lon</th>\n",
              "      <th>Elevation</th>\n",
              "      <th>Emissions</th>\n",
              "      <th>Forest</th>\n",
              "      <th>Roads</th>\n",
              "      <th>...</th>\n",
              "      <th>Plumes_Med</th>\n",
              "      <th>Plumes_Low</th>\n",
              "      <th>Max_Temp</th>\n",
              "      <th>Max_Wind</th>\n",
              "      <th>Precip</th>\n",
              "      <th>Rel_Humidity</th>\n",
              "      <th>Wind_Dir</th>\n",
              "      <th>BLH</th>\n",
              "      <th>AOD</th>\n",
              "      <th>PM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1131_1</td>\n",
              "      <td>1131</td>\n",
              "      <td>1</td>\n",
              "      <td>36.840574</td>\n",
              "      <td>-121.366314</td>\n",
              "      <td>122.986002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>53.4</td>\n",
              "      <td>5.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>251.0</td>\n",
              "      <td>758.111816</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1131_100</td>\n",
              "      <td>1131</td>\n",
              "      <td>100</td>\n",
              "      <td>36.840574</td>\n",
              "      <td>-121.366314</td>\n",
              "      <td>122.986002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>64.1</td>\n",
              "      <td>3.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>308.0</td>\n",
              "      <td>1155.824219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1131_101</td>\n",
              "      <td>1131</td>\n",
              "      <td>101</td>\n",
              "      <td>36.840574</td>\n",
              "      <td>-121.366314</td>\n",
              "      <td>122.986002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>62.7</td>\n",
              "      <td>5.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>1162.292725</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1131_102</td>\n",
              "      <td>1131</td>\n",
              "      <td>102</td>\n",
              "      <td>36.840574</td>\n",
              "      <td>-121.366314</td>\n",
              "      <td>122.986002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>67.7</td>\n",
              "      <td>7.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>221.0</td>\n",
              "      <td>263.273407</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1131_104</td>\n",
              "      <td>1131</td>\n",
              "      <td>104</td>\n",
              "      <td>36.840574</td>\n",
              "      <td>-121.366314</td>\n",
              "      <td>122.986002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>60.1</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>314.0</td>\n",
              "      <td>1268.919434</td>\n",
              "      <td>0.097667</td>\n",
              "      <td>2.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0    Unique    Id  Day        Lat         Lon   Elevation  \\\n",
              "0           1    1131_1  1131    1  36.840574 -121.366314  122.986002   \n",
              "1           2  1131_100  1131  100  36.840574 -121.366314  122.986002   \n",
              "2           3  1131_101  1131  101  36.840574 -121.366314  122.986002   \n",
              "3           4  1131_102  1131  102  36.840574 -121.366314  122.986002   \n",
              "4           5  1131_104  1131  104  36.840574 -121.366314  122.986002   \n",
              "\n",
              "   Emissions  Forest  Roads  ...  Plumes_Med  Plumes_Low  Max_Temp  Max_Wind  \\\n",
              "0        0.0       0    0.0  ...           0           0      53.4       5.3   \n",
              "1        0.0       0    0.0  ...           0           0      64.1       3.6   \n",
              "2        0.0       0    0.0  ...           0           0      62.7       5.1   \n",
              "3        0.0       0    0.0  ...           0           0      67.7       7.9   \n",
              "4        0.0       0    0.0  ...           0           0      60.1       6.5   \n",
              "\n",
              "   Precip  Rel_Humidity  Wind_Dir          BLH       AOD   PM  \n",
              "0     0.0          67.0     251.0   758.111816       NaN  3.7  \n",
              "1     0.0          48.0     308.0  1155.824219       NaN  2.6  \n",
              "2     0.0          64.0     200.0  1162.292725       NaN  2.9  \n",
              "3     0.0          68.0     221.0   263.273407       NaN  0.5  \n",
              "4     0.0          44.0     314.0  1268.919434  0.097667  2.4  \n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"../Data/datasets/Train.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOzPCGex_h8m"
      },
      "source": [
        "# Crossvalidate\n",
        "Make tables of crossvalidated predictions by station using Autogluon \\\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7jBfZMOYn0eh"
      },
      "outputs": [],
      "source": [
        "def cv(df):\n",
        "    n_fold = len(set(df['Id']))\n",
        "    kf = GroupKFold(n_fold)\n",
        "    split = kf.split(df, groups=df['Id'])\n",
        "\n",
        "    metric_df = pd.DataFrame(columns=['station', 'Day', 'PM', 'PM_pred', 'rmse', 'bias'])\n",
        "\n",
        "    for i, (train_idx, test_idx) in enumerate(split):\n",
        "        print(f'Starting training fold {i}.')\n",
        "        _ = gc.collect()\n",
        "\n",
        "        # Calculate the daily mean PM for each day, excluding the current validation set\n",
        "        validation_station = df.loc[test_idx, 'Id'].iloc[0]\n",
        "        train_df = df[~df['Id'].isin([validation_station])]\n",
        "        daily_mean_PM_excl_validation = train_df.groupby('Day')['PM'].mean()\n",
        "\n",
        "        # Add the daily mean PM (excluding validation station) as a feature to the original df\n",
        "        df['Daily_Mean_PM_Excl_Val'] = df['Day'].map(daily_mean_PM_excl_validation)\n",
        "\n",
        "        features = ['Day', 'Lat', 'Lon', 'Elevation', 'Emissions', 'Forest',\n",
        "                    'Roads', 'Streets', 'Plumes_High', 'Plumes_Med', 'Plumes_Low',\n",
        "                    'Max_Temp', 'Max_Wind', 'Precip', 'Rel_Humidity', 'Wind_Dir', 'BLH',\n",
        "                    'AOD', 'Daily_Mean_PM_Excl_Val']  # Include the new feature\n",
        "\n",
        "        # Adjust y by subtracting the mean PM for each day (excluding validation station)\n",
        "        df['Adjusted_PM'] = df['PM'] - df['Daily_Mean_PM_Excl_Val']\n",
        "        label_column = 'Adjusted_PM'\n",
        "\n",
        "        X = df[features]\n",
        "        y = df[label_column]\n",
        "\n",
        "        # Ensure the index alignment between X and y when slicing\n",
        "        train_data = TabularDataset(pd.concat([X.loc[train_idx], y.loc[train_idx]], axis=1))\n",
        "        test_data = TabularDataset(pd.concat([X.loc[test_idx], y.loc[test_idx]], axis=1))\n",
        "\n",
        "        predictor = TabularPredictor(label=label_column, eval_metric='r2').fit(train_data=train_data)\n",
        "        \n",
        "        # task.fit(train_data=train_data, \n",
        "        #                     label=label_column, \n",
        "        #                     hyperparameters=hyperparameters, \n",
        "        #                     # time_limits = 60*60, # un-comment these for the tuned neural network. Train for 1 hour. \n",
        "        #                     # hyperparameter_tune=True, \n",
        "        #                     eval_metric='r2')\n",
        "\n",
        "        y_test = test_data[label_column]\n",
        "        test_data_nolab = test_data.drop(labels=[label_column], axis=1)\n",
        "        y_pred = predictor.predict(test_data_nolab)\n",
        "\n",
        "        # Calculate metrics\n",
        "        bias = y_pred - y_test\n",
        "        rmse = np.abs(bias)\n",
        "\n",
        "        station = np.repeat(df.loc[test_idx]['Id'].iloc[0], len(test_data['Day']))\n",
        "        df_to_append = pd.DataFrame({'station': station, 'Day': test_data['Day'], 'PM': y_test + df.loc[test_idx, 'Daily_Mean_PM_Excl_Val'], 'PM_pred': y_pred + df.loc[test_idx, 'Daily_Mean_PM_Excl_Val'], 'rmse': rmse, 'bias': bias})\n",
        "\n",
        "        metric_df = metric_df.append(df_to_append, ignore_index=True)\n",
        "\n",
        "    return metric_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PViC1H3rPYk",
        "outputId": "8503af8b-8743-4fae-a23d-ee37c19df1cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240208_230446\"\n",
            "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
            "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
            "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
            "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
            "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
            "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240208_230446\"\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.0.0\n",
            "Python Version:     3.8.18\n",
            "Operating System:   Darwin\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   Darwin Kernel Version 20.3.0: Thu Jan 21 00:07:06 PST 2021; root:xnu-7195.81.3~1/RELEASE_X86_64\n",
            "CPU Count:          16\n",
            "Memory Avail:       12.71 GB / 32.00 GB (39.7%)\n",
            "Disk Space Avail:   39.69 GB / 931.55 GB (4.3%)\n",
            "===================================================\n",
            "Train Data Rows:    9573\n",
            "Train Data Columns: 19\n",
            "Label Column:       Adjusted_PM\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (157.36935483870968, -35.93064516129033, 0.0, 4.93395)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    13013.00 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.39 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training fold 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 14 | ['Lat', 'Lon', 'Elevation', 'Emissions', 'Roads', ...]\n",
            "\t\t('int', [])   :  5 | ['Day', 'Forest', 'Plumes_High', 'Plumes_Med', 'Plumes_Low']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 14 | ['Lat', 'Lon', 'Elevation', 'Emissions', 'Roads', ...]\n",
            "\t\t('int', [])       :  1 | ['Day']\n",
            "\t\t('int', ['bool']) :  4 | ['Forest', 'Plumes_High', 'Plumes_Med', 'Plumes_Low']\n",
            "\t0.2s = Fit runtime\n",
            "\t19 features in original data used to generate 19 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.13 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.24s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 8615, Val Rows: 958\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.1778\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.2123\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/envs/updated_autogluon/lib/python3.8/site-packages/lightgbm/lib/lib_lightgbm.so, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
            "  Referenced from: /opt/anaconda3/envs/updated_autogluon/lib/python3.8/site-packages/lightgbm/lib/lib_lightgbm.so\n",
            "  Reason: image not found\n",
            "Fitting model: LightGBM ...\n",
            "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/envs/updated_autogluon/lib/python3.8/site-packages/lightgbm/lib/lib_lightgbm.so, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
            "  Referenced from: /opt/anaconda3/envs/updated_autogluon/lib/python3.8/site-packages/lightgbm/lib/lib_lightgbm.so\n",
            "  Reason: image not found\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t0.3511\t = Validation score   (r2)\n",
            "\t2.55s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t0.7239\t = Validation score   (r2)\n",
            "\t78.2s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t0.6318\t = Validation score   (r2)\n",
            "\t1.12s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t0.6272\t = Validation score   (r2)\n",
            "\t14.91s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t0.6225\t = Validation score   (r2)\n",
            "\t9.8s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t0.6406\t = Validation score   (r2)\n",
            "\t45.63s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/opt/anaconda3/envs/updated_autogluon/lib/python3.8/site-packages/lightgbm/lib/lib_lightgbm.so, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
            "  Referenced from: /opt/anaconda3/envs/updated_autogluon/lib/python3.8/site-packages/lightgbm/lib/lib_lightgbm.so\n",
            "  Reason: image not found\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\tEnsemble Weights: {'CatBoost': 0.762, 'NeuralNetTorch': 0.125, 'NeuralNetFastAI': 0.088, 'KNeighborsUnif': 0.025}\n",
            "\t0.7312\t = Validation score   (r2)\n",
            "\t0.72s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 154.96s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240208_230446\")\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'append'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/1f/_ptk0jz93h39qj25crwwtb0w0000gn/T/ipykernel_51007/2775259725.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAutoGluon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mAutoGluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data/output/CV/AutoGluon_spatial_var.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/1f/_ptk0jz93h39qj25crwwtb0w0000gn/T/ipykernel_51007/2860526094.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mstation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mdf_to_append\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'station'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Day'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PM'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Daily_Mean_PM_Excl_Val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PM_pred'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Daily_Mean_PM_Excl_Val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rmse'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bias'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mmetric_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_to_append\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetric_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/anaconda3/envs/updated_autogluon/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
          ]
        }
      ],
      "source": [
        "AutoGluon = cv(df)\n",
        "AutoGluon.to_csv(\"Data/output/CV/AutoGluon_spatial_var.csv\", index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7q5JjrToJeJ"
      },
      "source": [
        "# Consolidate dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQmbdO-MoIWJ"
      },
      "outputs": [],
      "source": [
        "small_ids = pd.read_csv(\"Data/datasets/Small_Ids.csv\")['x']\n",
        "small_ids = [float(i) for i in small_ids]\n",
        "\n",
        "full_df = pd.DataFrame(columns = ['Model', 'test_area', 'station', 'Day', 'PM', 'PM_pred'])\n",
        "\n",
        "paths = glob.glob(\"Data/output/CV/*.csv\")\n",
        "\n",
        "for path in paths:\n",
        "  f = os.path.split(path)[1]\n",
        "  Model = os.path.splitext(f)[0]\n",
        "  Table = pd.read_csv(path)\n",
        "  Table['test_area'] = Table['station'].isin(small_ids)\n",
        "  Table['Model'] = Model\n",
        "\n",
        "  full_df = full_df.append(Table, ignore_index = True)\n",
        "\n",
        "full_df.to_csv(\"Data/output/all_cv_preds.csv\", index = False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
